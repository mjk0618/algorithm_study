{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 체크포인트 로드\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 로드\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    eos_token=\"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 입력값 만들기\n",
    "# encode() 메서드는 입력 문장을 토큰화한 뒤 정수로 인덱싱하는 역할 수행\n",
    "input_ids = tokenizer.encode(\"안녕하세요\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25906,  8702,  7801,  8084]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리디 서치로 문장 생성\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,  # 확률값이 높은 단어를 다음 단어로 결정\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 토큰 인덱스를 문장으로 복원하기\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그\n"
     ]
    }
   ],
   "source": [
    "# 빔 서치\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        num_beams=3,  # 빔 크기를 지정\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\" 하고 나는 물었다.\n",
      "\"그건 뭐죠?\" 나는 물었다.\n",
      "나는 대답하지 않았다.\n",
      "\"그런데 왜 그걸 물어요? 그건 무슨 뜻이에요?\n"
     ]
    }
   ],
   "source": [
    "# 반복 줄이기 1\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 반복 줄이기 2\n",
    "# repetition penalty가 1.0이면, greedy search와 다르지 않음\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.0,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"아니요, 저는요.\"\n",
      "\"그럼, 그건 무슨 말씀이신지요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# repetition_penalty=1.1\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요, 아저씨. 저는 지금 이 순간에도 괜찮아요.\"\n",
      "\"그래서 오늘은 제가 할 수 있는 일이 무엇인지 말해 보겠습니다.\"\n",
      "\"이제\n"
     ]
    }
   ],
   "source": [
    "# repetition_penalty=1.2\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요, 아저씨. 저는 지금 이 순간에도 괜찮아요. 그리고 제가 할 수 있는 일은 아무것도 없어요.\n",
      "이제 그만 돌아가고 싶어요.\n",
      "제가 하는 일이 무엇\n"
     ]
    }
   ],
   "source": [
    "# repetition_penalty=1.5\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.5,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요, 아저씨. 저는 지금 이 순간에도 괜찮아요. 그리고 제가 할 수 있는 일은 아무것도 없어요.\n",
      "이제 그만 돌아가고 싶어요.\n",
      "제가 하는 일이 무엇\n"
     ]
    }
   ],
   "source": [
    "# 탑-k 샘플링: 확률 상 상위에 위치하는 k개의 토큰 중 하나를 랜덤으로 취함\n",
    "with torch.no_grad():\n",
    "    generate_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=50,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 템퍼러쳐 스케일링: 확률 분포를 대소 관계의 역전 없이 분포의 모양만을 바꿔서 문장을 다양하게 생성하는 기법\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=50,\n",
    "        temperature=0.01,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요\"라는 제목으로 쓴 사진입니다.\n",
      "그는 북한 주민들의 눈까지 수그러져 있다고 강조하면서도 북한 당국은 탈선을 의심하기는 할 수밖에요.\n",
      "미 연방우주청은 이미 그 가능성을 거론하기는 한 발 이른 모습들을 취합니다.\n",
      "대개의 일반 시민이 북한에서\n"
     ]
    }
   ],
   "source": [
    "# 템퍼러쳐 값이 1보다 커질수록 다양한 문장이 생성되지만 품질은 나빠질 수 있다\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=50,\n",
    "        temperature=100000000.0,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요!!!!!!!\n",
      "#강아지스타그램 #멍스타그램 #펫스타그램 #petstagram #dogstagram #doggy #포메라니\n"
     ]
    }
   ],
   "source": [
    "# 탑-p 샘플링(nucleus sampling)\n",
    "# 확률값이 높은 순서대로 내림차순 정렬을 한 뒤 누적 확률값이 p 이상인 최소 개수의 토큰 집합 가운데 하나를 선택\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_p=0.92,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요...\n",
      "제가 좀 미안해요..\n",
      "저도 너무 #기분좋은한하루였어요 ㅠㅠㅠㅠ \n",
      "그리고 이번에 너무 좋은 하루 보냈으니 내년에도 기분좋은하루 보내세요!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top-p=0.01\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_p=0.92,\n",
    "    )\n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종합 적용\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.5,\n",
    "        no_repeat_ngram_size=3,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.92,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"라고 묻는다면 이 또한 당신은 '내 생각'일 것이다.\n",
      "그리고 그 답변들은 대부분 거짓말이다.\n",
      "그것은 마치 '이렇게 해야 한다'는 것이 아니라, '그렇다'의 뜻임을 깨닫지 못한 채 침묵하기만\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf74378b3249af5359f07a9caa84b7a763805fcaf59590a4dc4a6d39ea6824f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
