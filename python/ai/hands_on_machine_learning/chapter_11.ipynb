{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __11. 심층 신경망 훈련하기__\n",
    "\n",
    "__11.1 그레이디언트 소실(vanishing gradient)과 폭주(exploding gradient)__\n",
    "\n",
    "역전파 알고리즘은 출력층에서 입력층으로 오차 그레이디언트를 전파하면서 진행된다. <br>\n",
    "알고리즘이 신경망의 모든 파라미터에 대한 오차 함수의 그레이디언트를 계산하면, <br>\n",
    "경사 하강법 단계에서 이 그레이디언트를 사용하여 각 파라미터를 수정한다. <br>\n",
    "그런데 알고리즘이 하위층으로 진행될수록 그레이디언트가 점점 작아지는 경우가 많다. <br>\n",
    "경사 하강법이 하위층의 연결 가중치를 변경되지 않은 채로 둔다면 훈련이 수렴하지 않을 것이다. <br>\n",
    "이를 그레이디언트 소실이라고 한다. 반대로 그레이디언트가 점점 커져서 여러 층이 비정상적으로 <br>\n",
    "큰 가중치로 갱신되면 알고리즘은 발산하고, 이를 그레이디언트 폭주라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    " \n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1e87b1f8400>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# He 초기화\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x1e68da33a60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fan_avg기반의 균등분포 He 초기화\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode=\"fan_avg\", distribution=\"uniform\")\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeakyReLU\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 1.2854 - accuracy: 0.6040 - val_loss: 0.8754 - val_accuracy: 0.7156\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7918 - accuracy: 0.7417 - val_loss: 0.7076 - val_accuracy: 0.7726\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6815 - accuracy: 0.7741 - val_loss: 0.6400 - val_accuracy: 0.7894\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.6220 - accuracy: 0.7927 - val_loss: 0.5878 - val_accuracy: 0.8058\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5836 - accuracy: 0.8049 - val_loss: 0.5569 - val_accuracy: 0.8176\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5563 - accuracy: 0.8127 - val_loss: 0.5344 - val_accuracy: 0.8258\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5354 - accuracy: 0.8186 - val_loss: 0.5155 - val_accuracy: 0.8306\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5193 - accuracy: 0.8236 - val_loss: 0.5077 - val_accuracy: 0.8326\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5063 - accuracy: 0.8266 - val_loss: 0.4911 - val_accuracy: 0.8354\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4950 - accuracy: 0.8297 - val_loss: 0.4836 - val_accuracy: 0.8414\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 1.3341 - accuracy: 0.6185 - val_loss: 0.8914 - val_accuracy: 0.7180\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.8043 - accuracy: 0.7407 - val_loss: 0.7148 - val_accuracy: 0.7682\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6904 - accuracy: 0.7732 - val_loss: 0.6452 - val_accuracy: 0.7888\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6305 - accuracy: 0.7930 - val_loss: 0.5916 - val_accuracy: 0.8086\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5914 - accuracy: 0.8057 - val_loss: 0.5596 - val_accuracy: 0.8188\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5632 - accuracy: 0.8127 - val_loss: 0.5354 - val_accuracy: 0.8258\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5417 - accuracy: 0.8199 - val_loss: 0.5158 - val_accuracy: 0.8324\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5248 - accuracy: 0.8243 - val_loss: 0.5066 - val_accuracy: 0.8374\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5110 - accuracy: 0.8281 - val_loss: 0.4891 - val_accuracy: 0.8396\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4990 - accuracy: 0.8321 - val_loss: 0.4811 - val_accuracy: 0.8442\n"
     ]
    }
   ],
   "source": [
    "# PReLU 테스트\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.PReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELU\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 정규화\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화 층은 입력마다 $\\gamma, \\beta, \\mu, \\sigma$ 를 추가한다. <br>\n",
    "첫 번째 배치 정규화 층은 $4 \\times 784 = 3136$개의 파라미터가 있다. <br>\n",
    "$\\mu, \\sigma$ 는 이동 평균인데, 역전파로 학습되지 않기 때문에 케라스는 Non-trainable 파라미터로 뷴류한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그레이디언트 클리핑: 역전파될 때 일정 임곗값을 넘어서지 못하게 그레이디언트를 잘라내는 것\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__11.2 사전 훈련된 층 재사용하기__\n",
    "\n",
    "일반적으로 아주 큰 규모의 DNN을 처음부터 새로 훈련하는 것은 좋은 생각이 아니다. <br>\n",
    "해결하려는 것과 비슷한 유형의 문제를 처리한 신경망이 이미 있는지 찾아본 다음, 그 신경망의 하위층을 재사용한다. <br>\n",
    "이를 전이학습(transfer learning)이라고 한다. 이 방법은 훈련 속도도 크게 높이고 필요한 훈련 데이터도 크게 줄여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6)  # 샌들 혹은 셔츠\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2  # 클래스 인덱스 7, 8, 9를 5, 6, 7로 이동\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32)  # 이진분류(셔츠: 클래스6인가?)\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,\n",
       "       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 7s 4ms/step - loss: 0.5740 - accuracy: 0.8123 - val_loss: 0.3812 - val_accuracy: 0.8657\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3535 - accuracy: 0.8767 - val_loss: 0.3204 - val_accuracy: 0.8869\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3164 - accuracy: 0.8881 - val_loss: 0.2951 - val_accuracy: 0.9006\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2973 - accuracy: 0.8962 - val_loss: 0.2830 - val_accuracy: 0.9076\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2845 - accuracy: 0.9015 - val_loss: 0.2743 - val_accuracy: 0.9088\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2747 - accuracy: 0.9056 - val_loss: 0.2659 - val_accuracy: 0.9086\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2667 - accuracy: 0.9080 - val_loss: 0.2666 - val_accuracy: 0.9113\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2600 - accuracy: 0.9112 - val_loss: 0.2583 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2546 - accuracy: 0.9130 - val_loss: 0.2541 - val_accuracy: 0.9158\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2498 - accuracy: 0.9146 - val_loss: 0.2510 - val_accuracy: 0.9150\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2453 - accuracy: 0.9165 - val_loss: 0.2468 - val_accuracy: 0.9180\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2414 - accuracy: 0.9181 - val_loss: 0.2470 - val_accuracy: 0.9160\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2380 - accuracy: 0.9191 - val_loss: 0.2438 - val_accuracy: 0.9155\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 5s 4ms/step - loss: 0.2346 - accuracy: 0.9201 - val_loss: 0.2406 - val_accuracy: 0.9190\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2314 - accuracy: 0.9211 - val_loss: 0.2462 - val_accuracy: 0.9145\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2285 - accuracy: 0.9218 - val_loss: 0.2405 - val_accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 6s 5ms/step - loss: 0.2260 - accuracy: 0.9244 - val_loss: 0.2392 - val_accuracy: 0.9175\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2232 - accuracy: 0.9240 - val_loss: 0.2447 - val_accuracy: 0.9150\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2207 - accuracy: 0.9248 - val_loss: 0.2330 - val_accuracy: 0.9220\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2185 - accuracy: 0.9255 - val_loss: 0.2328 - val_accuracy: 0.9213\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                      validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kang MinJae\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 55ms/step - loss: 0.6126 - accuracy: 0.6250 - val_loss: 0.4924 - val_accuracy: 0.7617\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 0.4299 - accuracy: 0.7850 - val_loss: 0.3758 - val_accuracy: 0.8469\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.3362 - accuracy: 0.8850 - val_loss: 0.3117 - val_accuracy: 0.8935\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.2799 - accuracy: 0.9300 - val_loss: 0.2669 - val_accuracy: 0.9209\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2376 - accuracy: 0.9450 - val_loss: 0.2344 - val_accuracy: 0.9402\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 0.2072 - accuracy: 0.9500 - val_loss: 0.2111 - val_accuracy: 0.9483\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1848 - accuracy: 0.9550 - val_loss: 0.1946 - val_accuracy: 0.9554\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1675 - accuracy: 0.9650 - val_loss: 0.1783 - val_accuracy: 0.9604\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1524 - accuracy: 0.9700 - val_loss: 0.1622 - val_accuracy: 0.9686\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1381 - accuracy: 0.9700 - val_loss: 0.1514 - val_accuracy: 0.9706\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.1275 - accuracy: 0.9800 - val_loss: 0.1427 - val_accuracy: 0.9696\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1184 - accuracy: 0.9800 - val_loss: 0.1352 - val_accuracy: 0.9706\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1106 - accuracy: 0.9900 - val_loss: 0.1285 - val_accuracy: 0.9706\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.1040 - accuracy: 0.9850 - val_loss: 0.1224 - val_accuracy: 0.9736\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0978 - accuracy: 0.9900 - val_loss: 0.1164 - val_accuracy: 0.9757\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0925 - accuracy: 0.9900 - val_loss: 0.1120 - val_accuracy: 0.9767\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0878 - accuracy: 0.9900 - val_loss: 0.1077 - val_accuracy: 0.9787\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.0835 - accuracy: 0.9950 - val_loss: 0.1041 - val_accuracy: 0.9787\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0795 - accuracy: 0.9950 - val_loss: 0.1004 - val_accuracy: 0.9797\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 0.0759 - accuracy: 0.9950 - val_loss: 0.0976 - val_accuracy: 0.9807\n"
     ]
    }
   ],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_1 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전이학습\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.0519 - accuracy: 0.9950 - val_loss: 0.0631 - val_accuracy: 0.9929\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0497 - accuracy: 0.9950 - val_loss: 0.0611 - val_accuracy: 0.9919\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 0.0477 - accuracy: 0.9950 - val_loss: 0.0592 - val_accuracy: 0.9919\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0459 - accuracy: 0.9950 - val_loss: 0.0575 - val_accuracy: 0.9919\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 39ms/step - loss: 0.0441 - accuracy: 0.9950 - val_loss: 0.0559 - val_accuracy: 0.9919\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0425 - accuracy: 0.9950 - val_loss: 0.0545 - val_accuracy: 0.9919\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0411 - accuracy: 0.9950 - val_loss: 0.0531 - val_accuracy: 0.9919\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0398 - accuracy: 0.9950 - val_loss: 0.0519 - val_accuracy: 0.9919\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0385 - accuracy: 0.9950 - val_loss: 0.0507 - val_accuracy: 0.9919\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0374 - accuracy: 0.9950 - val_loss: 0.0497 - val_accuracy: 0.9919\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0363 - accuracy: 0.9950 - val_loss: 0.0491 - val_accuracy: 0.9919\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0353 - accuracy: 0.9950 - val_loss: 0.0481 - val_accuracy: 0.9919\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0343 - accuracy: 0.9950 - val_loss: 0.0471 - val_accuracy: 0.9919\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0333 - accuracy: 0.9950 - val_loss: 0.0463 - val_accuracy: 0.9919\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0325 - accuracy: 0.9950 - val_loss: 0.0455 - val_accuracy: 0.9919\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0316 - accuracy: 0.9950 - val_loss: 0.0448 - val_accuracy: 0.9919\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.0308 - accuracy: 0.9950 - val_loss: 0.0440 - val_accuracy: 0.9919\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0300 - accuracy: 0.9950 - val_loss: 0.0433 - val_accuracy: 0.9919\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0292 - accuracy: 0.9950 - val_loss: 0.0426 - val_accuracy: 0.9919\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.0285 - accuracy: 0.9950 - val_loss: 0.0420 - val_accuracy: 0.9919\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0916 - accuracy: 0.9855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09157665073871613, 0.9854999780654907]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 5ms/step - loss: 0.0368 - accuracy: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03683459758758545, 0.9944999814033508]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__11.3 고속 옵티마이저__\n",
    "\n",
    "훈련 속도를 높이는 네 가지 방법을 보았다.\n",
    "- 연결 가중치에 좋은 초기화 전략 적용하기\n",
    "- 좋은 활성화 함수 사용하기\n",
    "- 배치 정규화 사용하기\n",
    "- 보조 작업 또는 비지도 학습을 사용하여 만들 수 있는 사전훈련된 네트워크의 일부 재사용하기\n",
    "\n",
    "한 가지 또 다른 방법은 표준적인 경사하강법 옵티마이저 대신 더 빠른 옵티마이저를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모멘텀 최적화\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네스테로프 가속 경사\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaGrad\n",
    "# 적응적 학습률(Adaptive learning rate): 경사가 완만한 차원보다 가파른 차원에 대해 더 빠르게 감소\n",
    "# 너무 빨리 느려져서 전역 최적점에 수렴하지 못하는 위험이 있다\n",
    "optimizer = keras.optimizers.Adagrad(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam(Adaptive Moment Estimation: 적응적 모멘트 추정)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaMax\n",
    "optimizer = keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nadam\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 스케줄링\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지수 기반 스케줄링\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1 ** (epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클로저를 사용하여 eta_0(lr)과 s를 설정\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "# history = model.fit(X_train_sccaled, y_train, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구간별 고정 스케줄링\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 기반 스케줄링\n",
    "# 다섯 번의 에포크 동안 검증 손실이 향상되지 않으면 학습률에 0.5를 곱함\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__11.4 규제를 사용해 과대적합 피하기__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연결 가중치에 규제 강도 0.01을 사용하여 l2 규제 적용\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 드롭아웃\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 몬테 카를로 드롭아웃(MC Dropout)\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.61, 0.01, 0.01, 0.03, 0.05, 0.01, 0.08, 0.06, 0.  , 0.15]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.66, 0.  , 0.  , 0.04, 0.03, 0.  , 0.03, 0.01, 0.  , 0.23]],\n",
       "\n",
       "       [[0.7 , 0.01, 0.01, 0.01, 0.04, 0.  , 0.01, 0.2 , 0.01, 0.01]],\n",
       "\n",
       "       [[0.49, 0.05, 0.03, 0.01, 0.05, 0.  , 0.03, 0.27, 0.  , 0.07]],\n",
       "\n",
       "       [[0.98, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01]],\n",
       "\n",
       "       [[0.28, 0.01, 0.03, 0.44, 0.07, 0.01, 0.05, 0.01, 0.  , 0.11]],\n",
       "\n",
       "       [[0.87, 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.01, 0.  , 0.09]],\n",
       "\n",
       "       [[0.17, 0.01, 0.06, 0.03, 0.01, 0.  , 0.1 , 0.02, 0.  , 0.59]],\n",
       "\n",
       "       [[0.44, 0.  , 0.  , 0.04, 0.15, 0.2 , 0.05, 0.06, 0.01, 0.05]],\n",
       "\n",
       "       [[0.1 , 0.02, 0.02, 0.3 , 0.05, 0.09, 0.13, 0.09, 0.01, 0.18]],\n",
       "\n",
       "       [[0.82, 0.  , 0.01, 0.02, 0.02, 0.  , 0.05, 0.03, 0.  , 0.05]],\n",
       "\n",
       "       [[0.07, 0.  , 0.  , 0.03, 0.39, 0.02, 0.16, 0.03, 0.  , 0.3 ]],\n",
       "\n",
       "       [[0.64, 0.  , 0.02, 0.02, 0.  , 0.  , 0.01, 0.11, 0.  , 0.2 ]],\n",
       "\n",
       "       [[0.32, 0.02, 0.01, 0.13, 0.03, 0.01, 0.1 , 0.03, 0.01, 0.35]],\n",
       "\n",
       "       [[0.29, 0.01, 0.  , 0.16, 0.15, 0.04, 0.17, 0.04, 0.  , 0.14]],\n",
       "\n",
       "       [[0.87, 0.  , 0.03, 0.01, 0.  , 0.  , 0.  , 0.06, 0.01, 0.02]],\n",
       "\n",
       "       [[0.32, 0.02, 0.  , 0.04, 0.08, 0.02, 0.26, 0.01, 0.  , 0.26]],\n",
       "\n",
       "       [[0.58, 0.  , 0.  , 0.03, 0.08, 0.01, 0.3 , 0.01, 0.  , 0.  ]],\n",
       "\n",
       "       [[0.36, 0.  , 0.03, 0.08, 0.2 , 0.04, 0.07, 0.06, 0.02, 0.14]],\n",
       "\n",
       "       [[0.69, 0.01, 0.02, 0.02, 0.11, 0.  , 0.01, 0.01, 0.  , 0.12]],\n",
       "\n",
       "       [[0.23, 0.02, 0.06, 0.11, 0.01, 0.  , 0.03, 0.12, 0.  , 0.42]],\n",
       "\n",
       "       [[0.61, 0.  , 0.  , 0.06, 0.03, 0.02, 0.01, 0.1 , 0.01, 0.15]],\n",
       "\n",
       "       [[0.18, 0.  , 0.  , 0.06, 0.  , 0.  , 0.  , 0.11, 0.  , 0.65]],\n",
       "\n",
       "       [[0.73, 0.  , 0.05, 0.01, 0.01, 0.  , 0.08, 0.08, 0.  , 0.04]],\n",
       "\n",
       "       [[0.06, 0.  , 0.  , 0.06, 0.76, 0.  , 0.01, 0.01, 0.  , 0.1 ]],\n",
       "\n",
       "       [[0.4 , 0.01, 0.  , 0.01, 0.28, 0.04, 0.07, 0.09, 0.01, 0.08]],\n",
       "\n",
       "       [[0.95, 0.  , 0.  , 0.  , 0.01, 0.01, 0.  , 0.02, 0.  , 0.01]],\n",
       "\n",
       "       [[0.46, 0.  , 0.01, 0.13, 0.25, 0.01, 0.08, 0.02, 0.  , 0.02]],\n",
       "\n",
       "       [[0.5 , 0.  , 0.  , 0.01, 0.03, 0.01, 0.44, 0.  , 0.  , 0.01]],\n",
       "\n",
       "       [[0.22, 0.  , 0.02, 0.05, 0.  , 0.16, 0.13, 0.11, 0.02, 0.29]],\n",
       "\n",
       "       [[0.31, 0.01, 0.  , 0.04, 0.15, 0.01, 0.23, 0.07, 0.  , 0.19]],\n",
       "\n",
       "       [[0.44, 0.  , 0.  , 0.01, 0.04, 0.  , 0.04, 0.08, 0.  , 0.39]],\n",
       "\n",
       "       [[0.88, 0.  , 0.01, 0.01, 0.02, 0.  , 0.01, 0.02, 0.  , 0.04]],\n",
       "\n",
       "       [[0.88, 0.  , 0.  , 0.01, 0.01, 0.  , 0.01, 0.06, 0.  , 0.02]],\n",
       "\n",
       "       [[0.8 , 0.01, 0.01, 0.03, 0.01, 0.01, 0.07, 0.02, 0.  , 0.04]],\n",
       "\n",
       "       [[0.71, 0.  , 0.  , 0.  , 0.02, 0.  , 0.02, 0.04, 0.  , 0.21]],\n",
       "\n",
       "       [[0.85, 0.01, 0.  , 0.03, 0.04, 0.  , 0.  , 0.  , 0.  , 0.07]],\n",
       "\n",
       "       [[0.68, 0.03, 0.03, 0.02, 0.04, 0.01, 0.05, 0.1 , 0.04, 0.  ]],\n",
       "\n",
       "       [[0.36, 0.  , 0.02, 0.01, 0.05, 0.01, 0.02, 0.19, 0.  , 0.34]],\n",
       "\n",
       "       [[0.29, 0.  , 0.03, 0.16, 0.06, 0.01, 0.01, 0.04, 0.  , 0.4 ]],\n",
       "\n",
       "       [[0.13, 0.11, 0.  , 0.1 , 0.04, 0.02, 0.03, 0.05, 0.01, 0.51]],\n",
       "\n",
       "       [[0.57, 0.  , 0.06, 0.11, 0.02, 0.02, 0.05, 0.07, 0.02, 0.08]],\n",
       "\n",
       "       [[0.41, 0.01, 0.  , 0.01, 0.02, 0.  , 0.14, 0.07, 0.  , 0.33]],\n",
       "\n",
       "       [[0.09, 0.01, 0.  , 0.14, 0.34, 0.28, 0.04, 0.02, 0.  , 0.07]],\n",
       "\n",
       "       [[0.73, 0.01, 0.  , 0.01, 0.07, 0.  , 0.05, 0.  , 0.  , 0.13]],\n",
       "\n",
       "       [[0.45, 0.01, 0.07, 0.01, 0.04, 0.01, 0.16, 0.02, 0.  , 0.22]],\n",
       "\n",
       "       [[0.02, 0.01, 0.19, 0.1 , 0.1 , 0.02, 0.02, 0.21, 0.03, 0.29]],\n",
       "\n",
       "       [[0.5 , 0.03, 0.  , 0.  , 0.  , 0.  , 0.05, 0.24, 0.05, 0.11]],\n",
       "\n",
       "       [[0.24, 0.  , 0.02, 0.17, 0.01, 0.  , 0.22, 0.06, 0.  , 0.28]],\n",
       "\n",
       "       [[0.96, 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.  , 0.02]],\n",
       "\n",
       "       [[0.15, 0.01, 0.19, 0.38, 0.05, 0.02, 0.14, 0.03, 0.  , 0.03]],\n",
       "\n",
       "       [[0.06, 0.  , 0.  , 0.01, 0.32, 0.01, 0.06, 0.01, 0.  , 0.52]],\n",
       "\n",
       "       [[0.4 , 0.09, 0.  , 0.07, 0.03, 0.  , 0.13, 0.2 , 0.01, 0.07]],\n",
       "\n",
       "       [[0.17, 0.12, 0.04, 0.03, 0.08, 0.  , 0.26, 0.08, 0.  , 0.22]],\n",
       "\n",
       "       [[0.06, 0.  , 0.1 , 0.09, 0.06, 0.21, 0.32, 0.08, 0.01, 0.07]],\n",
       "\n",
       "       [[0.11, 0.02, 0.  , 0.01, 0.03, 0.  , 0.12, 0.13, 0.  , 0.57]],\n",
       "\n",
       "       [[0.72, 0.  , 0.03, 0.04, 0.14, 0.02, 0.01, 0.02, 0.  , 0.02]],\n",
       "\n",
       "       [[0.68, 0.  , 0.  , 0.04, 0.  , 0.  , 0.04, 0.03, 0.  , 0.21]],\n",
       "\n",
       "       [[0.89, 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.06, 0.01, 0.01]],\n",
       "\n",
       "       [[0.29, 0.01, 0.  , 0.01, 0.06, 0.  , 0.01, 0.01, 0.  , 0.61]],\n",
       "\n",
       "       [[0.54, 0.  , 0.01, 0.  , 0.  , 0.  , 0.06, 0.09, 0.01, 0.28]],\n",
       "\n",
       "       [[0.85, 0.  , 0.  , 0.01, 0.01, 0.  , 0.08, 0.01, 0.  , 0.04]],\n",
       "\n",
       "       [[0.64, 0.  , 0.  , 0.09, 0.02, 0.01, 0.13, 0.02, 0.  , 0.09]],\n",
       "\n",
       "       [[0.31, 0.  , 0.03, 0.16, 0.12, 0.  , 0.12, 0.02, 0.  , 0.24]],\n",
       "\n",
       "       [[0.71, 0.01, 0.  , 0.  , 0.07, 0.  , 0.  , 0.02, 0.  , 0.18]],\n",
       "\n",
       "       [[0.76, 0.01, 0.  , 0.03, 0.  , 0.  , 0.01, 0.07, 0.  , 0.11]],\n",
       "\n",
       "       [[0.18, 0.03, 0.26, 0.32, 0.07, 0.  , 0.02, 0.1 , 0.01, 0.03]],\n",
       "\n",
       "       [[0.43, 0.04, 0.  , 0.01, 0.02, 0.  , 0.02, 0.3 , 0.01, 0.17]],\n",
       "\n",
       "       [[0.22, 0.  , 0.04, 0.16, 0.11, 0.09, 0.11, 0.17, 0.01, 0.09]],\n",
       "\n",
       "       [[0.27, 0.  , 0.03, 0.03, 0.02, 0.02, 0.07, 0.28, 0.02, 0.25]],\n",
       "\n",
       "       [[0.34, 0.01, 0.  , 0.04, 0.06, 0.01, 0.27, 0.09, 0.03, 0.15]],\n",
       "\n",
       "       [[0.85, 0.  , 0.01, 0.  , 0.02, 0.  , 0.  , 0.04, 0.  , 0.09]],\n",
       "\n",
       "       [[0.66, 0.  , 0.01, 0.17, 0.1 , 0.01, 0.03, 0.01, 0.  , 0.01]],\n",
       "\n",
       "       [[0.11, 0.  , 0.01, 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.86]],\n",
       "\n",
       "       [[0.83, 0.  , 0.  , 0.02, 0.  , 0.  , 0.02, 0.1 , 0.  , 0.02]],\n",
       "\n",
       "       [[0.03, 0.  , 0.03, 0.05, 0.18, 0.01, 0.16, 0.04, 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.17, 0.03, 0.06, 0.09, 0.06, 0.04, 0.22, 0.08, 0.01, 0.25]],\n",
       "\n",
       "       [[0.09, 0.01, 0.  , 0.03, 0.06, 0.  , 0.09, 0.12, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.01, 0.  , 0.  , 0.  , 0.04, 0.01, 0.62, 0.28, 0.01, 0.02]],\n",
       "\n",
       "       [[0.38, 0.  , 0.01, 0.  , 0.  , 0.01, 0.07, 0.34, 0.  , 0.19]],\n",
       "\n",
       "       [[0.4 , 0.  , 0.01, 0.05, 0.01, 0.02, 0.03, 0.35, 0.  , 0.13]],\n",
       "\n",
       "       [[0.75, 0.  , 0.05, 0.04, 0.01, 0.  , 0.  , 0.07, 0.  , 0.07]],\n",
       "\n",
       "       [[0.53, 0.  , 0.01, 0.02, 0.01, 0.01, 0.03, 0.07, 0.  , 0.31]],\n",
       "\n",
       "       [[0.26, 0.  , 0.06, 0.06, 0.1 , 0.04, 0.33, 0.06, 0.01, 0.08]],\n",
       "\n",
       "       [[0.79, 0.01, 0.  , 0.04, 0.05, 0.  , 0.05, 0.02, 0.01, 0.02]],\n",
       "\n",
       "       [[0.9 , 0.  , 0.  , 0.  , 0.02, 0.  , 0.  , 0.03, 0.  , 0.04]],\n",
       "\n",
       "       [[0.24, 0.  , 0.03, 0.26, 0.09, 0.03, 0.13, 0.1 , 0.  , 0.13]],\n",
       "\n",
       "       [[0.48, 0.  , 0.  , 0.01, 0.07, 0.01, 0.17, 0.03, 0.01, 0.21]],\n",
       "\n",
       "       [[0.79, 0.  , 0.  , 0.  , 0.04, 0.  , 0.06, 0.01, 0.  , 0.1 ]],\n",
       "\n",
       "       [[0.94, 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.01, 0.  , 0.03]],\n",
       "\n",
       "       [[0.23, 0.  , 0.05, 0.02, 0.02, 0.  , 0.07, 0.15, 0.  , 0.46]],\n",
       "\n",
       "       [[0.9 , 0.  , 0.  , 0.  , 0.  , 0.01, 0.01, 0.02, 0.01, 0.04]],\n",
       "\n",
       "       [[0.07, 0.01, 0.03, 0.02, 0.31, 0.07, 0.08, 0.33, 0.  , 0.09]],\n",
       "\n",
       "       [[0.16, 0.  , 0.01, 0.12, 0.08, 0.01, 0.15, 0.25, 0.  , 0.21]],\n",
       "\n",
       "       [[0.04, 0.01, 0.04, 0.6 , 0.11, 0.1 , 0.04, 0.04, 0.01, 0.02]],\n",
       "\n",
       "       [[0.64, 0.  , 0.  , 0.03, 0.01, 0.04, 0.06, 0.07, 0.  , 0.14]],\n",
       "\n",
       "       [[0.21, 0.01, 0.05, 0.05, 0.07, 0.04, 0.04, 0.07, 0.04, 0.42]],\n",
       "\n",
       "       [[0.09, 0.  , 0.01, 0.63, 0.01, 0.02, 0.12, 0.01, 0.  , 0.11]],\n",
       "\n",
       "       [[0.84, 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.12, 0.  , 0.02]],\n",
       "\n",
       "       [[0.7 , 0.02, 0.  , 0.01, 0.03, 0.  , 0.01, 0.01, 0.  , 0.22]],\n",
       "\n",
       "       [[0.83, 0.01, 0.  , 0.03, 0.08, 0.  , 0.04, 0.01, 0.  , 0.  ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47, 0.01, 0.02, 0.07, 0.07, 0.02, 0.08, 0.08, 0.01, 0.18]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29, 0.02, 0.04, 0.11, 0.1 , 0.04, 0.1 , 0.08, 0.01, 0.18]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0942"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)\n",
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.core.dense.Dense at 0x11bda583dc0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 맥스 노름 규제\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                   kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf74378b3249af5359f07a9caa84b7a763805fcaf59590a4dc4a6d39ea6824f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
